{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import shap\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias in COMPAS\n",
    "\n",
    "In this section, we will study if the COMPAS model is biased by comparing the output scores with the real rate of recividism. In other words, given two individuals with the same features except race, we will try to analyze if the model overpredicts a higher score for a given race. \n",
    "\n",
    "COMPAS works by evaluating a range of factors including age, sex, personality traits, measures of social isolation, prior criminal history, family criminality, geography, and employment status. Northpointe gets some of this information from criminal records, and the rest from a questionnaire that asks defendants to respond to queries like, “How many of your friends/acquaintances are taking drugs illegally?” and to agree or disagree with statements like, “A hungry person has a right to steal.”\n",
    "\n",
    "COMPAS returns a score from 0 to 10 indicating the risk of recividism. In order to compare more easily, the decimal score will be transformed to a binary label indicating High risk (5-10) or Low risk (1-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv'\n",
    "df = pd.read_csv(url)\n",
    "df['high_risk'] = (df['decile_score'] >= 5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>compas_screening_date</th>\n",
       "      <th>sex</th>\n",
       "      <th>dob</th>\n",
       "      <th>age</th>\n",
       "      <th>age_cat</th>\n",
       "      <th>race</th>\n",
       "      <th>juv_fel_count</th>\n",
       "      <th>decile_score</th>\n",
       "      <th>juv_misd_count</th>\n",
       "      <th>juv_other_count</th>\n",
       "      <th>priors_count</th>\n",
       "      <th>days_b_screening_arrest</th>\n",
       "      <th>c_jail_in</th>\n",
       "      <th>c_jail_out</th>\n",
       "      <th>c_case_number</th>\n",
       "      <th>c_offense_date</th>\n",
       "      <th>c_arrest_date</th>\n",
       "      <th>c_days_from_compas</th>\n",
       "      <th>c_charge_degree</th>\n",
       "      <th>c_charge_desc</th>\n",
       "      <th>is_recid</th>\n",
       "      <th>r_case_number</th>\n",
       "      <th>r_charge_degree</th>\n",
       "      <th>r_days_from_arrest</th>\n",
       "      <th>r_offense_date</th>\n",
       "      <th>r_charge_desc</th>\n",
       "      <th>r_jail_in</th>\n",
       "      <th>r_jail_out</th>\n",
       "      <th>violent_recid</th>\n",
       "      <th>is_violent_recid</th>\n",
       "      <th>vr_case_number</th>\n",
       "      <th>vr_charge_degree</th>\n",
       "      <th>vr_offense_date</th>\n",
       "      <th>vr_charge_desc</th>\n",
       "      <th>type_of_assessment</th>\n",
       "      <th>decile_score.1</th>\n",
       "      <th>score_text</th>\n",
       "      <th>screening_date</th>\n",
       "      <th>v_type_of_assessment</th>\n",
       "      <th>v_decile_score</th>\n",
       "      <th>v_score_text</th>\n",
       "      <th>v_screening_date</th>\n",
       "      <th>in_custody</th>\n",
       "      <th>out_custody</th>\n",
       "      <th>priors_count.1</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>event</th>\n",
       "      <th>two_year_recid</th>\n",
       "      <th>high_risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>miguel hernandez</td>\n",
       "      <td>miguel</td>\n",
       "      <td>hernandez</td>\n",
       "      <td>2013-08-14</td>\n",
       "      <td>Male</td>\n",
       "      <td>1947-04-18</td>\n",
       "      <td>69</td>\n",
       "      <td>Greater than 45</td>\n",
       "      <td>Other</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2013-08-13 06:03:42</td>\n",
       "      <td>2013-08-14 05:41:20</td>\n",
       "      <td>13011352CF10A</td>\n",
       "      <td>2013-08-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>F</td>\n",
       "      <td>Aggravated Assault w/Firearm</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Risk of Recidivism</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-08-14</td>\n",
       "      <td>Risk of Violence</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-08-14</td>\n",
       "      <td>2014-07-07</td>\n",
       "      <td>2014-07-14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>327</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>kevon dixon</td>\n",
       "      <td>kevon</td>\n",
       "      <td>dixon</td>\n",
       "      <td>2013-01-27</td>\n",
       "      <td>Male</td>\n",
       "      <td>1982-01-22</td>\n",
       "      <td>34</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>African-American</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2013-01-26 03:45:27</td>\n",
       "      <td>2013-02-05 05:36:53</td>\n",
       "      <td>13001275CF10A</td>\n",
       "      <td>2013-01-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>F</td>\n",
       "      <td>Felony Battery w/Prior Convict</td>\n",
       "      <td>1</td>\n",
       "      <td>13009779CF10A</td>\n",
       "      <td>(F3)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-07-05</td>\n",
       "      <td>Felony Battery (Dom Strang)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>13009779CF10A</td>\n",
       "      <td>(F3)</td>\n",
       "      <td>2013-07-05</td>\n",
       "      <td>Felony Battery (Dom Strang)</td>\n",
       "      <td>Risk of Recidivism</td>\n",
       "      <td>3</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-01-27</td>\n",
       "      <td>Risk of Violence</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-01-27</td>\n",
       "      <td>2013-01-26</td>\n",
       "      <td>2013-02-05</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>ed philo</td>\n",
       "      <td>ed</td>\n",
       "      <td>philo</td>\n",
       "      <td>2013-04-14</td>\n",
       "      <td>Male</td>\n",
       "      <td>1991-05-14</td>\n",
       "      <td>24</td>\n",
       "      <td>Less than 25</td>\n",
       "      <td>African-American</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2013-04-13 04:58:34</td>\n",
       "      <td>2013-04-14 07:02:04</td>\n",
       "      <td>13005330CF10A</td>\n",
       "      <td>2013-04-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>F</td>\n",
       "      <td>Possession of Cocaine</td>\n",
       "      <td>1</td>\n",
       "      <td>13011511MM10A</td>\n",
       "      <td>(M1)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013-06-16</td>\n",
       "      <td>Driving Under The Influence</td>\n",
       "      <td>2013-06-16</td>\n",
       "      <td>2013-06-16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Risk of Recidivism</td>\n",
       "      <td>4</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-04-14</td>\n",
       "      <td>Risk of Violence</td>\n",
       "      <td>3</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-04-14</td>\n",
       "      <td>2013-06-16</td>\n",
       "      <td>2013-06-16</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>marcu brown</td>\n",
       "      <td>marcu</td>\n",
       "      <td>brown</td>\n",
       "      <td>2013-01-13</td>\n",
       "      <td>Male</td>\n",
       "      <td>1993-01-21</td>\n",
       "      <td>23</td>\n",
       "      <td>Less than 25</td>\n",
       "      <td>African-American</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13000570CF10A</td>\n",
       "      <td>2013-01-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>F</td>\n",
       "      <td>Possession of Cannabis</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Risk of Recidivism</td>\n",
       "      <td>8</td>\n",
       "      <td>High</td>\n",
       "      <td>2013-01-13</td>\n",
       "      <td>Risk of Violence</td>\n",
       "      <td>6</td>\n",
       "      <td>Medium</td>\n",
       "      <td>2013-01-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1174</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>bouthy pierrelouis</td>\n",
       "      <td>bouthy</td>\n",
       "      <td>pierrelouis</td>\n",
       "      <td>2013-03-26</td>\n",
       "      <td>Male</td>\n",
       "      <td>1973-01-22</td>\n",
       "      <td>43</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>Other</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12014130CF10A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-01-09</td>\n",
       "      <td>76.0</td>\n",
       "      <td>F</td>\n",
       "      <td>arrest case no charge</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Risk of Recidivism</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-03-26</td>\n",
       "      <td>Risk of Violence</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-03-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1102</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                name   first         last compas_screening_date   sex  \\\n",
       "0   1    miguel hernandez  miguel    hernandez            2013-08-14  Male   \n",
       "1   3         kevon dixon   kevon        dixon            2013-01-27  Male   \n",
       "2   4            ed philo      ed        philo            2013-04-14  Male   \n",
       "3   5         marcu brown   marcu        brown            2013-01-13  Male   \n",
       "4   6  bouthy pierrelouis  bouthy  pierrelouis            2013-03-26  Male   \n",
       "\n",
       "          dob  age          age_cat              race  juv_fel_count  \\\n",
       "0  1947-04-18   69  Greater than 45             Other              0   \n",
       "1  1982-01-22   34          25 - 45  African-American              0   \n",
       "2  1991-05-14   24     Less than 25  African-American              0   \n",
       "3  1993-01-21   23     Less than 25  African-American              0   \n",
       "4  1973-01-22   43          25 - 45             Other              0   \n",
       "\n",
       "   decile_score  juv_misd_count  juv_other_count  priors_count  \\\n",
       "0             1               0                0             0   \n",
       "1             3               0                0             0   \n",
       "2             4               0                1             4   \n",
       "3             8               1                0             1   \n",
       "4             1               0                0             2   \n",
       "\n",
       "   days_b_screening_arrest            c_jail_in           c_jail_out  \\\n",
       "0                     -1.0  2013-08-13 06:03:42  2013-08-14 05:41:20   \n",
       "1                     -1.0  2013-01-26 03:45:27  2013-02-05 05:36:53   \n",
       "2                     -1.0  2013-04-13 04:58:34  2013-04-14 07:02:04   \n",
       "3                      NaN                  NaN                  NaN   \n",
       "4                      NaN                  NaN                  NaN   \n",
       "\n",
       "   c_case_number c_offense_date c_arrest_date  c_days_from_compas  \\\n",
       "0  13011352CF10A     2013-08-13           NaN                 1.0   \n",
       "1  13001275CF10A     2013-01-26           NaN                 1.0   \n",
       "2  13005330CF10A     2013-04-13           NaN                 1.0   \n",
       "3  13000570CF10A     2013-01-12           NaN                 1.0   \n",
       "4  12014130CF10A            NaN    2013-01-09                76.0   \n",
       "\n",
       "  c_charge_degree                   c_charge_desc  is_recid  r_case_number  \\\n",
       "0               F    Aggravated Assault w/Firearm         0            NaN   \n",
       "1               F  Felony Battery w/Prior Convict         1  13009779CF10A   \n",
       "2               F           Possession of Cocaine         1  13011511MM10A   \n",
       "3               F          Possession of Cannabis         0            NaN   \n",
       "4               F           arrest case no charge         0            NaN   \n",
       "\n",
       "  r_charge_degree  r_days_from_arrest r_offense_date  \\\n",
       "0             NaN                 NaN            NaN   \n",
       "1            (F3)                 NaN     2013-07-05   \n",
       "2            (M1)                 0.0     2013-06-16   \n",
       "3             NaN                 NaN            NaN   \n",
       "4             NaN                 NaN            NaN   \n",
       "\n",
       "                 r_charge_desc   r_jail_in  r_jail_out  violent_recid  \\\n",
       "0                          NaN         NaN         NaN            NaN   \n",
       "1  Felony Battery (Dom Strang)         NaN         NaN            NaN   \n",
       "2  Driving Under The Influence  2013-06-16  2013-06-16            NaN   \n",
       "3                          NaN         NaN         NaN            NaN   \n",
       "4                          NaN         NaN         NaN            NaN   \n",
       "\n",
       "   is_violent_recid vr_case_number vr_charge_degree vr_offense_date  \\\n",
       "0                 0            NaN              NaN             NaN   \n",
       "1                 1  13009779CF10A             (F3)      2013-07-05   \n",
       "2                 0            NaN              NaN             NaN   \n",
       "3                 0            NaN              NaN             NaN   \n",
       "4                 0            NaN              NaN             NaN   \n",
       "\n",
       "                vr_charge_desc  type_of_assessment  decile_score.1 score_text  \\\n",
       "0                          NaN  Risk of Recidivism               1        Low   \n",
       "1  Felony Battery (Dom Strang)  Risk of Recidivism               3        Low   \n",
       "2                          NaN  Risk of Recidivism               4        Low   \n",
       "3                          NaN  Risk of Recidivism               8       High   \n",
       "4                          NaN  Risk of Recidivism               1        Low   \n",
       "\n",
       "  screening_date v_type_of_assessment  v_decile_score v_score_text  \\\n",
       "0     2013-08-14     Risk of Violence               1          Low   \n",
       "1     2013-01-27     Risk of Violence               1          Low   \n",
       "2     2013-04-14     Risk of Violence               3          Low   \n",
       "3     2013-01-13     Risk of Violence               6       Medium   \n",
       "4     2013-03-26     Risk of Violence               1          Low   \n",
       "\n",
       "  v_screening_date  in_custody out_custody  priors_count.1  start   end  \\\n",
       "0       2013-08-14  2014-07-07  2014-07-14               0      0   327   \n",
       "1       2013-01-27  2013-01-26  2013-02-05               0      9   159   \n",
       "2       2013-04-14  2013-06-16  2013-06-16               4      0    63   \n",
       "3       2013-01-13         NaN         NaN               1      0  1174   \n",
       "4       2013-03-26         NaN         NaN               2      0  1102   \n",
       "\n",
       "   event  two_year_recid  high_risk  \n",
       "0      0               0          0  \n",
       "1      1               1          0  \n",
       "2      0               1          0  \n",
       "3      0               0          1  \n",
       "4      0               0          0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "As we don't have the input features needed to replicate the COMPAS model, we will train a classifier to predict the COMPAS score given the gender, race, age, priors_count, and crime factor. We will evaluate the model by using different fairness metrics, and study how different methods of data rebalancing can affect these metrics.\n",
    "\n",
    "\n",
    "SMOTE/Undersample/Oversample -> Train -> Evaluate different metrics\n",
    "\n",
    "### Metrics to evaluate:\n",
    "\n",
    "Castelnovo, A., Crupi, R., Greco, G., & Regoli, D. (2021). The zoo of Fairness metrics in Machine Learning. arXiv preprint arXiv:2106.00467.\n",
    "\n",
    "\n",
    "(INDEPENDENCE)\n",
    "\n",
    "- **Demographic parity**: Positive prediction ratio between two races.\n",
    "- **Demographic parity conditioned on priors?**\n",
    "\n",
    "(SEPARATION)\n",
    "\n",
    "- **Predictive equality** -> FPR\n",
    "- **Equality of opportunity** -> FNR\n",
    "\n",
    "(SUFFICIENCY)\n",
    "\n",
    "- **Predictive parity** -> Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fairness(y_pred, y_true, black_mask, white_mask):\n",
    "    y_pred_black = y_pred[black_mask]\n",
    "    y_true_black = y_true[black_mask]\n",
    "    y_pred_white = y_pred[white_mask]\n",
    "    y_true_white = y_true[white_mask]\n",
    "    # False Positive Rates FPR = FP / (FP + TN)\n",
    "    fpr_black = np.sum((y_pred_black == 1) * (y_true_black == 0)) / np.sum(y_true_black == 0)\n",
    "    fpr_white = np.sum((y_pred_white == 1) * (y_true_white == 0)) / np.sum(y_true_white == 0)\n",
    "    # True positive rates TPR = TP / (TP + FN)\n",
    "    tpr_black = np.sum((y_pred_black == 1)*(y_true_black == 1)) / np.sum(y_true_black == 1)\n",
    "    tpr_white = np.sum((y_pred_white == 1)*(y_true_white == 1)) / np.sum(y_true_white == 1)\n",
    "    # Precision\n",
    "    precision_black = precision_score(y_true_black, y_pred_black)\n",
    "    precision_white = precision_score(y_true_white, y_pred_white)\n",
    "\n",
    "    data = {}\n",
    "    data['TPR_w'] = tpr_white\n",
    "    data['TPR_b'] = tpr_black\n",
    "    data['FPR_w'] = fpr_white\n",
    "    data['FPR_b'] = fpr_black\n",
    "    data['Eq. Oportunity'] = abs(tpr_white-tpr_black)\n",
    "    data['Pred. Equality'] = abs(fpr_white-fpr_black)\n",
    "    data['Eq. odds'] = abs(tpr_white-tpr_black) + abs(fpr_white-fpr_black)\n",
    "    data['Accuracy'] = np.mean(y_pred == y_true)\n",
    "    \n",
    "    return data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE/Oversampling/Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_resampler(df, sampler=None, resample_test=False):\n",
    "\n",
    "    # Prepare the data\n",
    "    df_temp = df[(df['race'] == 'African-American') | (df['race'] == 'Caucasian')]\n",
    "    cols = ['age', 'sex', 'race', 'priors_count', 'score_text']\n",
    "    X, recid = df_temp[cols], df_temp['two_year_recid']\n",
    "    X['score_text'] = [0 if y_i == 'Low' else 1 for y_i in X['score_text']]\n",
    "    X = pd.get_dummies(X, drop_first=True)\n",
    "    X_train, X_test, recid_train, recid_test = train_test_split(X, recid.values, test_size=0.2, random_state=42)\n",
    "\n",
    "    ##############################\n",
    "    # RESAMPLE THE TRAINING SET  #\n",
    "    ##############################\n",
    "\n",
    "    # Build target variable combining both the race and whether it has recivided or not\n",
    "    #   - '00': Black, Non-recividist\n",
    "    #   - '01': Black, Recividist\n",
    "    #   - '10': White, Non-recividist\n",
    "    #   - '11': White, Recividist\n",
    "    if sampler:\n",
    "        # get the race value\n",
    "        y_race = X_train['race_Caucasian'].values\n",
    "        # build the target variable\n",
    "        y_sampler = np.array([str(a) + str(b) for a, b in zip(y_race, recid_train)])\n",
    "\n",
    "        print(\"TRAINING SET:\")\n",
    "        print(\"Before Sampling: \\n\\tBlack, Non-recidivist: {}\\n\\tBlack, Recidivist: {}\\\n",
    "            \\n\\tWhite, Non-recidivist: {}\\n\\tWhite, Recidivist: {}\".format(np.sum(y_sampler == '00'), \\\n",
    "            np.sum(y_sampler == '01'), np.sum(y_sampler == '10'), np.sum(y_sampler == '11')))\n",
    "\n",
    "        # Sample the dataset according to the race and the recividism rates\n",
    "        X_train, y_sampler = sampler.fit_resample(X_train, y_sampler)\n",
    "\n",
    "        print(\"After Sampling: \\n\\tBlack, Non-recidivist: {}\\n\\tBlack, Recidivist: {}\\\n",
    "            \\n\\tWhite, Non-recidivist: {}\\n\\tWhite, Recidivist: {}\".format(np.sum(y_sampler == '00'), \\\n",
    "            np.sum(y_sampler == '01'), np.sum(y_sampler == '10'), np.sum(y_sampler == '11')))\n",
    "\n",
    "        # Undo the label, i.e. get the race and the real recividism rate\n",
    "        race, recid_train = np.array([int(y_i[0]) for y_i in y_sampler]), np.array([int(y_i[1]) for y_i in y_sampler])\n",
    "        X_train['race_Caucasian'] = race \n",
    "        \n",
    "    X_train, y_train = X_train.drop(columns='score_text'), X_train['score_text']\n",
    "\n",
    "    ####################################\n",
    "    # RESAMPLE THE TEST SET (OPTIONAL) #\n",
    "    ####################################\n",
    "\n",
    "    if resample_test and sampler:\n",
    "    # get the race value\n",
    "        y_race = X_test['race_Caucasian'].values\n",
    "        # build the target variable\n",
    "        y_sampler = np.array([str(a) + str(b) for a, b in zip(y_race, recid_test)])\n",
    "\n",
    "        print(\"TEST SET:\")\n",
    "        print(\"Before Sampling: \\n\\tBlack, Non-recidivist: {}\\n\\tBlack, Recidivist: {}\\\n",
    "            \\n\\tWhite, Non-recidivist: {}\\n\\tWhite, Recidivist: {}\".format(np.sum(y_sampler == '00'), \\\n",
    "            np.sum(y_sampler == '01'), np.sum(y_sampler == '10'), np.sum(y_sampler == '11')))\n",
    "\n",
    "        # Sample the dataset according to the race and the recividism rates\n",
    "        X_test, y_sampler = sampler.fit_resample(X_test, y_sampler)\n",
    "\n",
    "        print(\"After Sampling: \\n\\tBlack, Non-recidivist: {}\\n\\tBlack, Recidivist: {}\\\n",
    "            \\n\\tWhite, Non-recidivist: {}\\n\\tWhite, Recidivist: {}\".format(np.sum(y_sampler == '00'), \\\n",
    "            np.sum(y_sampler == '01'), np.sum(y_sampler == '10'), np.sum(y_sampler == '11')))\n",
    "\n",
    "        # Undo the label, i.e. get the race and the real recividism rate\n",
    "        race, recid_test = np.array([int(y_i[0]) for y_i in y_sampler]), np.array([int(y_i[1]) for y_i in y_sampler])\n",
    "        X_test['race_Caucasian'] = race \n",
    "\n",
    "    X_test, y_test = X_test.drop(columns='score_text'), X_test['score_text']\n",
    "\n",
    "    # Train the model\n",
    "\n",
    "    clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    black_mask = X_test['race_Caucasian'] == 0\n",
    "    white_mask = X_test['race_Caucasian'] == 1\n",
    "\n",
    "    # Evaluate fairness metrics\n",
    "    data = eval_fairness(y_pred, recid_test, black_mask, white_mask)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TPR_w</th>\n",
       "      <th>TPR_b</th>\n",
       "      <th>FPR_w</th>\n",
       "      <th>FPR_b</th>\n",
       "      <th>Eq. Oportunity</th>\n",
       "      <th>Pred. Equality</th>\n",
       "      <th>Eq. odds</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Original Training - Original Test</th>\n",
       "      <td>0.355670</td>\n",
       "      <td>0.713542</td>\n",
       "      <td>0.171617</td>\n",
       "      <td>0.381089</td>\n",
       "      <td>0.357872</td>\n",
       "      <td>0.209472</td>\n",
       "      <td>0.567343</td>\n",
       "      <td>0.658537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMOTE Training - Original Test</th>\n",
       "      <td>0.340206</td>\n",
       "      <td>0.716146</td>\n",
       "      <td>0.178218</td>\n",
       "      <td>0.383954</td>\n",
       "      <td>0.375940</td>\n",
       "      <td>0.205736</td>\n",
       "      <td>0.581676</td>\n",
       "      <td>0.654472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMOTE Training - SMOTE Test</th>\n",
       "      <td>0.294271</td>\n",
       "      <td>0.716146</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>0.421875</td>\n",
       "      <td>0.203125</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.608073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oversampling Training - Original Test</th>\n",
       "      <td>0.396907</td>\n",
       "      <td>0.700521</td>\n",
       "      <td>0.214521</td>\n",
       "      <td>0.369628</td>\n",
       "      <td>0.303614</td>\n",
       "      <td>0.155106</td>\n",
       "      <td>0.458720</td>\n",
       "      <td>0.653659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oversampling Training - Oversampling Test</th>\n",
       "      <td>0.372396</td>\n",
       "      <td>0.700521</td>\n",
       "      <td>0.205729</td>\n",
       "      <td>0.380208</td>\n",
       "      <td>0.328125</td>\n",
       "      <td>0.174479</td>\n",
       "      <td>0.502604</td>\n",
       "      <td>0.621745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Undersampling Training - Original Test</th>\n",
       "      <td>0.371134</td>\n",
       "      <td>0.721354</td>\n",
       "      <td>0.188119</td>\n",
       "      <td>0.418338</td>\n",
       "      <td>0.350220</td>\n",
       "      <td>0.230219</td>\n",
       "      <td>0.580439</td>\n",
       "      <td>0.648780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Undersampling Training - Undersampling Test</th>\n",
       "      <td>0.371134</td>\n",
       "      <td>0.721649</td>\n",
       "      <td>0.226804</td>\n",
       "      <td>0.453608</td>\n",
       "      <td>0.350515</td>\n",
       "      <td>0.226804</td>\n",
       "      <td>0.577320</td>\n",
       "      <td>0.603093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                TPR_w     TPR_b     FPR_w  \\\n",
       "Original Training - Original Test            0.355670  0.713542  0.171617   \n",
       "SMOTE Training - Original Test               0.340206  0.716146  0.178218   \n",
       "SMOTE Training - SMOTE Test                  0.294271  0.716146  0.187500   \n",
       "Oversampling Training - Original Test        0.396907  0.700521  0.214521   \n",
       "Oversampling Training - Oversampling Test    0.372396  0.700521  0.205729   \n",
       "Undersampling Training - Original Test       0.371134  0.721354  0.188119   \n",
       "Undersampling Training - Undersampling Test  0.371134  0.721649  0.226804   \n",
       "\n",
       "                                                FPR_b  Eq. Oportunity  \\\n",
       "Original Training - Original Test            0.381089        0.357872   \n",
       "SMOTE Training - Original Test               0.383954        0.375940   \n",
       "SMOTE Training - SMOTE Test                  0.390625        0.421875   \n",
       "Oversampling Training - Original Test        0.369628        0.303614   \n",
       "Oversampling Training - Oversampling Test    0.380208        0.328125   \n",
       "Undersampling Training - Original Test       0.418338        0.350220   \n",
       "Undersampling Training - Undersampling Test  0.453608        0.350515   \n",
       "\n",
       "                                             Pred. Equality  Eq. odds  \\\n",
       "Original Training - Original Test                  0.209472  0.567343   \n",
       "SMOTE Training - Original Test                     0.205736  0.581676   \n",
       "SMOTE Training - SMOTE Test                        0.203125  0.625000   \n",
       "Oversampling Training - Original Test              0.155106  0.458720   \n",
       "Oversampling Training - Oversampling Test          0.174479  0.502604   \n",
       "Undersampling Training - Original Test             0.230219  0.580439   \n",
       "Undersampling Training - Undersampling Test        0.226804  0.577320   \n",
       "\n",
       "                                             Accuracy  \n",
       "Original Training - Original Test            0.658537  \n",
       "SMOTE Training - Original Test               0.654472  \n",
       "SMOTE Training - SMOTE Test                  0.608073  \n",
       "Oversampling Training - Original Test        0.653659  \n",
       "Oversampling Training - Oversampling Test    0.621745  \n",
       "Undersampling Training - Original Test       0.648780  \n",
       "Undersampling Training - Undersampling Test  0.603093  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "index= []\n",
    "\n",
    "index.append(\"Original Training - Original Test\")\n",
    "data.append(eval_resampler(df))\n",
    "index.append(\"SMOTE Training - Original Test\")\n",
    "data.append(eval_resampler(df, sampler=SMOTE(random_state=42)))\n",
    "index.append(\"SMOTE Training - SMOTE Test\")\n",
    "data.append(eval_resampler(df, sampler=SMOTE(random_state=42), resample_test=True))\n",
    "index.append(\"Oversampling Training - Original Test\")\n",
    "data.append(eval_resampler(df, sampler=RandomOverSampler(random_state=42)))\n",
    "index.append(\"Oversampling Training - Oversampling Test\")\n",
    "data.append(eval_resampler(df, sampler=RandomOverSampler(random_state=42), resample_test=True))\n",
    "index.append(\"Undersampling Training - Original Test\")\n",
    "data.append(eval_resampler(df, sampler=RandomUnderSampler(random_state=42)))\n",
    "index.append(\"Undersampling Training - Undersampling Test\")\n",
    "data.append(eval_resampler(df, sampler=RandomUnderSampler(random_state=42), resample_test=True))\n",
    "\n",
    "\n",
    "clear_output(wait=True)\n",
    "\n",
    "pd.DataFrame(data, index=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a different classifier for each race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "df_temp = df[(df['race'] == 'African-American') | (df['race'] == 'Caucasian')]\n",
    "cols = ['age', 'sex', 'race', 'priors_count', 'score_text']\n",
    "X, recid = df_temp[cols], df_temp['two_year_recid']\n",
    "X['score_text'] = [0 if y_i == 'Low' else 1 for y_i in X['score_text']]\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "X_train, X_test, recid_train, recid_test = train_test_split(X, recid.values, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a classifier for each race\n",
    "X_train_black, recid_train_black = X_train[X_train['race_Caucasian'] == 0], recid_train[X_train['race_Caucasian'] == 0]\n",
    "X_train_white, recid_train_white = X_train[X_train['race_Caucasian'] == 1], recid_train[X_train['race_Caucasian'] == 1]\n",
    "# Get score text in order to train\n",
    "X_train_black, y_train_black = X_train_black.drop(columns='score_text'), X_train_black['score_text']\n",
    "X_train_white, y_train_white = X_train_white.drop(columns='score_text'), X_train_white['score_text']\n",
    "\n",
    "clf_black = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "clf_white = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# Fit the models\n",
    "clf_black.fit(X_train_black, y_train_black)\n",
    "clf_white.fit(X_train_white, y_train_white)\n",
    "\n",
    "# Make predictions\n",
    "X_test_black, recid_test_black = X_test[X_test['race_Caucasian'] == 0], recid_test[X_test['race_Caucasian'] == 0]\n",
    "X_test_white, recid_test_white = X_test[X_test['race_Caucasian'] == 1], recid_test[X_test['race_Caucasian'] == 1]\n",
    "# Get score text in order to train\n",
    "X_test_black, y_test_black = X_test_black.drop(columns='score_text'), X_test_black['score_text']\n",
    "X_test_white, y_test_white = X_test_white.drop(columns='score_text'), X_test_white['score_text']\n",
    "\n",
    "y_pred_black = clf_black.predict(X_test_black)\n",
    "y_pred_white = clf_white.predict(X_test_white)\n",
    "y_pred = np.concatenate((y_pred_black, y_pred_white))\n",
    "recid_test = np.concatenate((recid_test_black, recid_test_white))\n",
    "black_mask = np.array([True]*len(y_pred_black) + [False]*len(y_pred_white))\n",
    "white_mask = np.array([False]*len(y_pred_black) + [True]*len(y_pred_white))\n",
    "\n",
    "index.append(\"Split by race\")\n",
    "data.append(eval_fairness(y_pred, recid_test, black_mask, white_mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing race attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# Train without the race variable\n",
    "df_temp = df[(df['race'] == 'African-American') | (df['race'] == 'Caucasian')]\n",
    "cols = ['age', 'sex', 'race', 'priors_count', 'score_text']\n",
    "X, recid = df_temp[cols], df_temp['two_year_recid']\n",
    "X['score_text'] = [0 if y_i == 'Low' else 1 for y_i in X['score_text']]\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "X_train, X_test, recid_train, recid_test = train_test_split(X, recid.values, test_size=0.2, random_state=42)\n",
    "\n",
    "# drop the race\n",
    "X_train, y_train = X_train.drop(columns=['race_Caucasian', 'score_text']), X_train['score_text']\n",
    "# Train the model without race\n",
    "clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = clf.predict(X_test.drop(columns=['race_Caucasian', 'score_text']))\n",
    "black_mask = X_test['race_Caucasian'] == 0\n",
    "white_mask = X_test['race_Caucasian'] == 1 \n",
    "\n",
    "index.append(\"Remove race attribute\")\n",
    "data.append(eval_fairness(y_pred, recid_test, black_mask, white_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TPR_w</th>\n",
       "      <th>TPR_b</th>\n",
       "      <th>FPR_w</th>\n",
       "      <th>FPR_b</th>\n",
       "      <th>Eq. Oportunity</th>\n",
       "      <th>Pred. Equality</th>\n",
       "      <th>Eq. odds</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Original Training - Original Test</th>\n",
       "      <td>0.356</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMOTE Training - Original Test</th>\n",
       "      <td>0.340</td>\n",
       "      <td>0.716</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMOTE Training - SMOTE Test</th>\n",
       "      <td>0.294</td>\n",
       "      <td>0.716</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oversampling Training - Original Test</th>\n",
       "      <td>0.397</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oversampling Training - Oversampling Test</th>\n",
       "      <td>0.372</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Undersampling Training - Original Test</th>\n",
       "      <td>0.371</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Undersampling Training - Undersampling Test</th>\n",
       "      <td>0.371</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.454</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Split by race</th>\n",
       "      <td>0.371</td>\n",
       "      <td>0.703</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.506</td>\n",
       "      <td>0.654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Remove race attribute</th>\n",
       "      <td>0.407</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.442</td>\n",
       "      <td>0.664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             TPR_w  TPR_b  FPR_w  FPR_b  \\\n",
       "Original Training - Original Test            0.356  0.714  0.172  0.381   \n",
       "SMOTE Training - Original Test               0.340  0.716  0.178  0.384   \n",
       "SMOTE Training - SMOTE Test                  0.294  0.716  0.188  0.391   \n",
       "Oversampling Training - Original Test        0.397  0.701  0.215  0.370   \n",
       "Oversampling Training - Oversampling Test    0.372  0.701  0.206  0.380   \n",
       "Undersampling Training - Original Test       0.371  0.721  0.188  0.418   \n",
       "Undersampling Training - Undersampling Test  0.371  0.722  0.227  0.454   \n",
       "Split by race                                0.371  0.703  0.198  0.372   \n",
       "Remove race attribute                        0.407  0.674  0.172  0.347   \n",
       "\n",
       "                                             Eq. Oportunity  Pred. Equality  \\\n",
       "Original Training - Original Test                     0.358           0.209   \n",
       "SMOTE Training - Original Test                        0.376           0.206   \n",
       "SMOTE Training - SMOTE Test                           0.422           0.203   \n",
       "Oversampling Training - Original Test                 0.304           0.155   \n",
       "Oversampling Training - Oversampling Test             0.328           0.174   \n",
       "Undersampling Training - Original Test                0.350           0.230   \n",
       "Undersampling Training - Undersampling Test           0.351           0.227   \n",
       "Split by race                                         0.332           0.174   \n",
       "Remove race attribute                                 0.267           0.175   \n",
       "\n",
       "                                             Eq. odds  Accuracy  \n",
       "Original Training - Original Test               0.567     0.659  \n",
       "SMOTE Training - Original Test                  0.582     0.654  \n",
       "SMOTE Training - SMOTE Test                     0.625     0.608  \n",
       "Oversampling Training - Original Test           0.459     0.654  \n",
       "Oversampling Training - Oversampling Test       0.503     0.622  \n",
       "Undersampling Training - Original Test          0.580     0.649  \n",
       "Undersampling Training - Undersampling Test     0.577     0.603  \n",
       "Split by race                                   0.506     0.654  \n",
       "Remove race attribute                           0.442     0.664  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option('precision', 3)\n",
    "display(pd.DataFrame(data, index=index))\n",
    "pd.reset_option('precision')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness metrics\n",
    "\n",
    "It is extremely difficult to concretely define the concept of fairness. Because of this, there are many metrics associated with this concept. We can divide this metrics into two classes, whether they measure **individual** (similar individuals should have similar outcomes) or **group** fairness (the model shouldn't discriminate against certain groups). We will focus on the latter metrics. There are three broad notions of group fairness:\n",
    "\n",
    "- **Independence**: Decisions should be independent of any sensitive attribute.\n",
    "- **Separation**: Disparities on groups should be completely justified by the target variable, assuming that you can trust that the target variable is correctly labeled i.e. is not biased.\n",
    "- **Sufficiency**: Similar to separation, but taking into account the predicted variable instead of the target.\n",
    "\n",
    "### Independence\n",
    "\n",
    "Decisions should be independent from any protected attributes (demographic parity):\n",
    "\n",
    "$$P(\\hat{Y} = 1 \\mid A = a) = P(\\hat{Y} = 1 \\mid A = b), \\quad \\forall a, b \\in \\mathcal{A}$$\n",
    "\n",
    "There are situations where independence doesn't directly relate to fairness. For example, women may have a bigger admission rate into a degree than men because they might have better grades in general. In order to achieve independence, or demographic parity, the model **should favor the group** with lower admission rates, i.e. **treating groups differently**, and this might not be fair. For this reason, we have to be very careful with enforcing demographic parity, and it has to be justified, for example, we can't trust that the target variable is correctly labelled, or there is historical bias in the data.\n",
    "\n",
    "Another criteria for independence named **Conditional demographic parity** could be more suitable in most situations. In the last example, we could say that we have achieved conditional demographic parity if the predicted variable (admission) is independent of the protected attribute (sex) for individuals with the same grades.\n",
    "\n",
    "### Separation\n",
    "\n",
    "Disparities between groups should be completely justified by the target variable:\n",
    "\n",
    "$$P(\\hat{Y}=1 \\mid A = a, Y=y) = P(\\hat{Y} = 1 \\mid A = b, Y=y),\\quad \\forall a,b\\in \\mathcal{A}, y\\in{0, 1}$$\n",
    "\n",
    "This can be a good fairness metric **if the target variable is free of any bias**.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two relaxed versions of separation:\n",
    "\n",
    "- **Predictive equality**: Same false positive rates across groups. $FPR = \\dfrac{FP}{FP+TN}$\n",
    "- **Equality of opportunity**: Same false negative rates. $FNR = \\dfrac{FN}{FN + TP}$\n",
    "\n",
    "Depending on the problem, one may prioritize one or the other.\n",
    "\n",
    "- Predictive equality may be prioritized when you want to minimize the risk of innocent people being arrested (False positive) between groups.\n",
    "- Equality of opportunity may be prioritized when accepting people into a degree, i.e. both groups should have equal opportunities.\n",
    "\n",
    "In conclusion, separation might be a good fairness metric if the data can be trusted, specifically the target variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sufficiency\n",
    "\n",
    "When the model has the same precission across sensitive groups, it has achieved predictive parity:\n",
    "\n",
    "$$P(Y=1 \\mid A=a, \\hat{Y} = 1) = P(Y=1 \\mid A=b, \\hat{Y} = 1),\\quad \\forall a,b\\in \\mathcal{A}$$\n",
    "\n",
    "If we require the last condition to also hold true for $Y=0$, then the sufficiency criterion is satisfied.\n",
    "\n",
    "The main difference between sufficiency and separation is in the point of view: while in separation, the observations are grouped according to the target variable, in sufficiency they are grouped according to the predicted value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incompatibilities between metrics\n",
    "\n",
    "- If the target variable is binary and there is a group imbalance, then **independence** and **separation** are incompatible.\n",
    "- If there is a group imbalance, **sufficiency** and **independence** are also incompatible.\n",
    "- Separation and sufficiency can both hold either when there is an imbalance in sensitive groups.\n",
    "\n",
    "Chouldechova (2017) showed that, if the true recidivism rate is different for black and white people, then Predictive Parity and Equality of Odds cannot both hold, thus implying that a reflection on which of the two (in general of the many) notions is more important to be pursued in that specific case must be carefully considered."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c5e48f84046969b800ff52f6d80523bcd1ca3fb1a99f1449e4197bf6c73dc096"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
